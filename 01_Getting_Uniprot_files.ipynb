{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75342816",
   "metadata": {},
   "source": [
    "# Download uniprot IDs from curated human proteome\n",
    "\n",
    "<a rel=\"license\" href=\"https://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://licensebuttons.net/l/by-sa/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
    "\n",
    "Author: Dr Antonia Mey   \n",
    "Email: antonia.mey@ed.ac.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ed90c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import urllib\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "97fd2f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_uniprot_json_file(uni_prot_id, workdir = '.'):\n",
    "    #check if there is uniprot information available for the protein\n",
    "    try:\n",
    "        url_2 = 'https://www.uniprot.org/uniprot/' + uni_prot_id + '.json'\n",
    "        html_2 = urllib.request.urlopen(url_2)\n",
    "        lines = html_2.readlines()[0]\n",
    "        # now try and write to file\n",
    "        f = open(os.path.join(workdir,uni_prot_id)+'.json', 'w')\n",
    "        f.write(lines.decode('utf-8'))\n",
    "        f.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise Exception('Failed to obtain UNIPROT data. %s'%e)\n",
    "    \n",
    "\n",
    "    return html_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9b0f0cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kown_things(known_things,data):\n",
    "\n",
    "    #known_things['Helix'] = []\n",
    "    if 'features' in data.keys():\n",
    "        for d in data['features']:\n",
    "            #print(d['type'])\n",
    "            if d['type'] == 'Domain':\n",
    "                if d['description'] not in  known_things['Domain']:\n",
    "                    known_things['Domain'].append(d['description'])\n",
    "            elif d['type'] == 'Region':\n",
    "                if d['description'] not in  known_things['Region']:\n",
    "                    known_things['Region'].append( d['description'])\n",
    "            elif d['type'] == 'Motif':\n",
    "                if d['description'] not in  known_things['Motif']:\n",
    "                    known_things['Motif'].append(d['description'])\n",
    "    return known_things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fff25e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/unique_ids_from_spreadsheet.txt','w')\n",
    "for i in unique_ids:\n",
    "    f.write(f'{i}\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08538326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalise dictionary of all annotations for Domain, Region and Motif\n",
    "known_things = {}\n",
    "known_things['Domain'] = []\n",
    "known_things['Region'] = []\n",
    "known_things['Motif'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f8d6126c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At entry 12000/16238\n",
      "At entry 12500/16238\n",
      "At entry 13000/16238\n",
      "At entry 13500/16238\n",
      "At entry 14000/16238\n",
      "At entry 14500/16238\n",
      "At entry 15000/16238\n",
      "At entry 15500/16238\n",
      "At entry 16000/16238\n"
     ]
    }
   ],
   "source": [
    "# Loop over all uniprot IDs \n",
    "# This will take a few hours as it isn't parallelised!\n",
    "counter = 0\n",
    "for ids in unique_ids:\n",
    "    download_uniprot_json_file(ids, workdir='unique_ids')\n",
    "    # Process uniprot file\n",
    "    f = open('unique_ids/'+ids+'.json')\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    info = get_kown_things(known_things,data)\n",
    "    counter = counter +1\n",
    "    if counter%500==0:\n",
    "        print(f'At entry {counter}/{len(unique_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "70d3f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_keys = info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb206e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving all annotations for domain region and motif\n",
    "domain = list(set(info['Domain'])) # set generates unique list\n",
    "region = list(set(info['Region']))\n",
    "motif = list(set(info['Motif']))\n",
    "\n",
    "# Now lets save that information\n",
    "np.savetxt('data/motif.csv', motif, fmt='%s')\n",
    "np.savetxt('data/region.csv', region fmt='%s')\n",
    "np.savetxt('data/domain.csv', domain, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7bfa5e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we want to create an exclusion list file\n",
    "# 1. We exclude all domains\n",
    "# 2. We exclude certain regions\n",
    "# 3. we keep all motifs\n",
    "\n",
    "region_filter = []\n",
    "for d in region:\n",
    "    if 'PUM-HD' in d:\n",
    "        region_filter.append(d)\n",
    "    if 'HEAT' in d:\n",
    "        region_filter.append(d)\n",
    "    if 'ARM' in d:\n",
    "        region_filter.append(d) \n",
    "    if 'bHLH' in d:\n",
    "        region_filter.append(d)\n",
    "    #if 'BIG' in d:\n",
    "        region_filter.append(d)\n",
    "    #if 'helical'.lower() in d.lower():\n",
    "    #    if 'nonhelical'.lower() in d.lower():\n",
    "    #        continue\n",
    "    #    else:\n",
    "    #        region_filter.append(d)\n",
    "    #if 'helix'.lower() in d.lower():\n",
    "    #    region_filter.append(d)\n",
    "    if 'coil coil'.lower() in d.lower():\n",
    "        region_filter.append(d)\n",
    "    if 'DNA Binding'.lower() in d.lower():\n",
    "        region_filter.append(d)\n",
    "    if 'DNA-Binding'.lower() in d.lower():\n",
    "        region_filter.append(d)\n",
    "    if 'Leucine-zipper'.lower() in d.lower():\n",
    "        region_filter.append(d)\n",
    "np.savetxt('data/region_filter.csv', region_filter, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6139726f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bHLH\n",
      "BIG\n",
      "Pumilio\n"
     ]
    }
   ],
   "source": [
    "# Some old checking domain content\n",
    "for d in domain:\n",
    "    if 'PUM-HD'.lower() in d.lower():\n",
    "        print('Pumilio')\n",
    "    if 'HEAT'.lower() in d.lower():\n",
    "        print('HEAT')\n",
    "    if 'ARM'.lower() in d.lower():\n",
    "        print(d) \n",
    "    if 'bHLH'.lower() in d.lower():\n",
    "        print('bHLH')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd51071",
   "metadata": {},
   "source": [
    "# END\n",
    "------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
