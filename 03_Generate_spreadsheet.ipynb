{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1c073a",
   "metadata": {},
   "source": [
    "# Extract secondary structure from PDB files easily and generate spreadsheet\n",
    "\n",
    "<a rel=\"license\" href=\"https://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons Licence\" style=\"border-width:0\" src=\"https://licensebuttons.net/l/by-sa/4.0/88x31.png\" title='This work is licensed under a Creative Commons Attribution 4.0 International License.' align=\"right\"/></a>\n",
    "\n",
    "Author: Dr Antonia Mey   \n",
    "Email: antonia.mey@ed.ac.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51210d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob \n",
    "import os\n",
    "import urllib\n",
    "from collections import Counter\n",
    "import urllib\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eceb9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha_helix_length_and_location(secondary_struc, min_length=8):\n",
    "    helix_regions = []\n",
    "    counter = 0\n",
    "    curr_helix = []\n",
    "    indexes = [i for i, x in enumerate(list(secondary_struc)) if x == 'H']\n",
    "    for i in range(len(indexes)-1):\n",
    "        difference = indexes[i+1]-indexes[i]\n",
    "        if difference == 1:\n",
    "            curr_helix.append(indexes[i])\n",
    "            if i == len(indexes)-2:\n",
    "                if len(curr_helix)>=min_length-1:\n",
    "                    curr_helix.append(indexes[i+1])\n",
    "                    helix_regions.append(curr_helix)\n",
    "        else:\n",
    "            curr_helix.append(indexes[i])\n",
    "            if len(curr_helix)>=min_length-1:\n",
    "                helix_regions.append(curr_helix)\n",
    "            curr_helix = []\n",
    "    return helix_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3ee27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_serines(sequence):\n",
    "    n_serine = sequence.count('S')\n",
    "    if n_serine >=2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "378e385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_helices(helix_index_list, known_things, sequence_length, discard_list= ['bHLH', 'Leucine-zipper'],overlap_threshold =4):\n",
    "    '''\n",
    "    Parameters:\n",
    "    -----------\n",
    "    helix_index_list : list\n",
    "        2D list containing arrays of indexes where alpha helixes are\n",
    "    known_things : dictionary\n",
    "        extracted information from uniprot IDs around regions, domains and motives\n",
    "    discard_list : list of Strings\n",
    "        list that contains the identifiers to discard\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    helix_index_list : list\n",
    "        filtered list with correct helix indexes without the overlap\n",
    "    Algorithm description:\n",
    "    \n",
    "    - generate a boolean array of all False of length of the sequence\n",
    "    - for each annotation we want to check:\n",
    "    - Add True between start and end of domains/regions we want to check in the boolean array\n",
    "    - Then loop over helix list creating a boolean array of length sequence for each Helix section\n",
    "    - Use logic and to compare regions boolean array with helix boolean array. \n",
    "    - If the number of over lap of Trues is larger than theshold x, remove this helix chunch from list and don't write to spreadsheet.\n",
    "    '''\n",
    "    \n",
    "    bool_array =  np.zeros(sequence_length, dtype=bool)\n",
    "    # alpha_helix_index_list = get_alpha_helix_length_and_location(sec_struc_output, min_length=7)\n",
    "    helix_list_remove_index = []\n",
    "    for key in known_things.keys():\n",
    "        for entry in known_things[key]:\n",
    "            bool_array =  np.zeros(sequence_length, dtype=bool)\n",
    "            if entry[-1] in discard_list:\n",
    "                bool_array[entry[0]-1:entry[1]] = True\n",
    "            index = 0\n",
    "            for a in helix_index_list:\n",
    "                curr_helix =  np.zeros(sequence_length, dtype=bool)\n",
    "                curr_helix[a] = True\n",
    "                overalp = (np.sum(np.logical_and(bool_array, curr_helix)))\n",
    "                if overalp>overlap_threshold:\n",
    "                    helix_list_remove_index.append(index)\n",
    "                index = index+1\n",
    "    unique_idx = np.unique(helix_list_remove_index)\n",
    "    for index in sorted(unique_idx, reverse=True):\n",
    "        del helix_index_list[index]\n",
    "    return helix_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38e33270",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotating_helices(helix_index_list, known_things, sequence_length,overlap_threshold =4):\n",
    "    '''\n",
    "    Parameters:\n",
    "    -----------\n",
    "    helix_index_list : list\n",
    "        2D list containing arrays of indexes where alpha helixes are\n",
    "    known_things : dictionary\n",
    "        extracted information from uniprot IDs around regions, domains and motives\n",
    "    discard_list : list of Strings\n",
    "        list that contains the identifiers to discard\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    helix_index_list : list\n",
    "        filtered list with correct helix indexes without the overlap\n",
    "    Algorithm description:\n",
    "    \n",
    "    - generate a boolean array of all False of length of the sequence\n",
    "    - for each annotation we want to check:\n",
    "    - Add True between start and end of domains/regions we want to check in the boolean array\n",
    "    - Then loop over helix list creating a boolean array of length sequence for each Helix section\n",
    "    - Use logic and to compare regions boolean array with helix boolean array. \n",
    "    - If the number of over lap of Trues is larger than theshold x, remove this helix chunch from list and don't write to spreadsheet.\n",
    "    '''\n",
    "    annotations = [\"\"]*len(helix_index_list)\n",
    "    for key in known_things.keys():\n",
    "        for entry in known_things[key]:\n",
    "            # print(f'checking entry {entry}')\n",
    "            bool_array =  np.zeros(sequence_length, dtype=bool)\n",
    "            if entry[0] is None:\n",
    "                continue\n",
    "            elif entry[1] is None:\n",
    "                continue\n",
    "            bool_array[entry[0]-1:entry[1]] = True\n",
    "            index = 0\n",
    "            \n",
    "            for a in helix_index_list:\n",
    "                curr_helix =  np.zeros(sequence_length, dtype=bool)\n",
    "                curr_helix[a] = True\n",
    "                overalp = (np.sum(np.logical_and(bool_array, curr_helix)))\n",
    "                if overalp>overlap_threshold:\n",
    "                    # print(f'annotation found {entry[-1]}')\n",
    "                    annotations[index] = annotations[index] +key+':'+entry[-1]+\";\"\n",
    "                index = index+1\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ecdab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domainn_region_info(data):\n",
    "    known_things = {}\n",
    "    known_things['Domain'] = []\n",
    "    known_things['Region'] = []\n",
    "    known_things['Motif'] = []\n",
    "    #known_things['Helix'] = []\n",
    "    if 'features' in data.keys():\n",
    "        for d in data['features']:\n",
    "            #print(d['type'])\n",
    "            if d['type'] == 'Domain':\n",
    "                known_things['Domain'].append([d['location']['start']['value'], d['location']['end']['value'], d['description']])\n",
    "            elif d['type'] == 'Region':\n",
    "                known_things['Region'].append([d['location']['start']['value'], d['location']['end']['value'], d['description']])\n",
    "            elif d['type'] == 'Motif':\n",
    "                known_things['Motif'].append([d['location']['start']['value'], d['location']['end']['value'], d['description']])\n",
    "            #elif d['type'] == 'Helix':\n",
    "            #    known_things['Helix'].append([d['location']['start']['value'], d['location']['end']['value'], d['description']])\n",
    "\n",
    "            \n",
    "    return known_things       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "75f9f3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_discrad_list(domain_f_name, region_f_name):\n",
    "    # Getting domains\n",
    "    domain_filter = []\n",
    "    # This should go into a try thing!\n",
    "    f = open(domain_f_name)\n",
    "    f_content = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    for l in f_content:\n",
    "        domain_filter.append(l.strip()) \n",
    "        \n",
    "    # Getting regions\n",
    "    region_filter = []\n",
    "    f = open(region_f_name)\n",
    "    f_content = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    for l in f_content:\n",
    "        region_filter.append(l.strip())\n",
    "    discard_list = np.hstack((region_filter,domain_filter))\n",
    "    return discard_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d63068b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_table_rows_for_uniprot_id(uni_prot_id, df, work_dir = 'temp', verbose = False, annotate = False):\n",
    "    # print('Working on uniprot ID', uni_prot_id)\n",
    "    sec_struc_output = np.load('dssp_info/'+uni_prot_id+'.npy')\n",
    "    # print('got secondary structure')\n",
    "    #print(sec_struc_output)\n",
    "    \n",
    "    # Process uniprot file\n",
    "    f = open('unique_ids/'+uni_prot_id+'.json')\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    \n",
    "    # print('read json file successfully')\n",
    "    \n",
    "    # Some basics:\n",
    "    # checking keys\n",
    "    keys = data.keys()\n",
    "    \n",
    "    # Primary Accession\n",
    "    uni_id = 'NA'\n",
    "    if 'primaryAccession' in keys:\n",
    "        uni_id = data['primaryAccession'] \n",
    "        if verbose:\n",
    "             print('uni_id',uni_id)\n",
    "    else: \n",
    "        print('Issue: No primary Accession found')\n",
    "    \n",
    "    # Genes\n",
    "    gene_name = 'NA'\n",
    "    if 'genes' in keys:\n",
    "        gene_name = data['genes'][0]['geneName']['value']\n",
    "        if verbose:\n",
    "            print('gene_name',gene_name)\n",
    "    else:\n",
    "        print('Issue: No gene found found')\n",
    "    \n",
    "    # getting the nucleotide sequenceID\n",
    "    ref_seq_id = 'NA'\n",
    "    if 'uniProtKBCrossReferences' in keys:\n",
    "        for db in data['uniProtKBCrossReferences']:\n",
    "            if db['database'] == \"RefSeq\":\n",
    "                if '.' in db['properties'][0]['value']:\n",
    "                    ref_seq_id = db['properties'][0]['value'].split('.')[0]\n",
    "                else:\n",
    "                    ref_seq_id = db['properties'][0]['value']\n",
    "    if verbose:\n",
    "        print('ref_seq_id',ref_seq_id)\n",
    "    \n",
    "    # getting sequence info\n",
    "    sequence = 'NA'\n",
    "    seq_length = 0\n",
    "    if 'sequence' in keys:\n",
    "        sequence = data['sequence']['value']\n",
    "        if verbose:\n",
    "            print('sequence',sequence)\n",
    "    \n",
    "        seq_length = data['sequence']['length']\n",
    "        if seq_length != len(sequence):\n",
    "            print(f\"Warning for Uniprot id {uni_prot_id} sequence length recorded is not same as actual sequence\")\n",
    "\n",
    "    if seq_length == len(sec_struc_output):\n",
    "\n",
    "        \n",
    "\n",
    "        # Now lets get the helices:\n",
    "        alpha_helix_index_list = get_alpha_helix_length_and_location(sec_struc_output, min_length=8)\n",
    "\n",
    "        # Get the information of what domains and regions exist in the uniport id file:\n",
    "        known_things = get_domainn_region_info(data)\n",
    "\n",
    "        discard_list = ['PUM-HD', 'HEAT', 'ARM', 'bHLH', 'Leucine-zipper']\n",
    "        discard_list = get_discrad_list('data/domain.csv', 'data/region_filter.csv')\n",
    "        discard_list = []\n",
    "\n",
    "        # Missing: Filter the alpha_helix_index_list\n",
    "        filtered_helix_list=filter_helices(alpha_helix_index_list, known_things, seq_length, discard_list=discard_list,overlap_threshold =4)\n",
    "\n",
    "        # annotate helices:\n",
    "        if annotate:\n",
    "            filtered_helix_list = alpha_helix_index_list\n",
    "            annotations = annotating_helices(filtered_helix_list, known_things, seq_length,overlap_threshold =4)\n",
    "\n",
    "        for i in range(len(filtered_helix_list)):\n",
    "            firstAA_position_in_HELIDR = filtered_helix_list[i][0]+1\n",
    "            lastAA_position_in_HELIDR = filtered_helix_list[i][-1]+1\n",
    "            HELIDR_seq = sequence[filtered_helix_list[i][0]:filtered_helix_list[i][-1]+1]\n",
    "            down_stream_seq = ''\n",
    "            up_stream_seq = ''\n",
    "            if filtered_helix_list[i][0]-10 >= 0 and filtered_helix_list[i][-1]+11 < len(sequence):\n",
    "                # Note the +2 here does not include the last helix AA. \n",
    "                # We have to make sure here that we check the arrays are not out of bounds!\n",
    "                down_stream_seq = sequence[filtered_helix_list[i][-1]+1:filtered_helix_list[i][-1]+11]\n",
    "                up_stream_seq = sequence[filtered_helix_list[i][0]-10:filtered_helix_list[i][0]]\n",
    "            elif alpha_helix_index_list[i][0]-10 <= 0:\n",
    "                # Do we want a shorter version? \n",
    "                down_stream_seq = sequence[filtered_helix_list[i][-1]+1:filtered_helix_list[i][-1]+11]\n",
    "                up_stream_seq = sequence[0:filtered_helix_list[i][0]]\n",
    "            elif alpha_helix_index_list[i][-1]+11 > len(sequence):\n",
    "                down_stream_seq = sequence[filtered_helix_list[i][-1]+2:len(sequence)+1]\n",
    "                up_stream_seq = sequence[filtered_helix_list[i][0]-10:filtered_helix_list[i][0]]\n",
    "\n",
    "            # This may need fixing if we have shorter upstream and downstream strings\n",
    "            if len(down_stream_seq)==10 and len(up_stream_seq)==10: \n",
    "                Two_S5P_down = count_serines(down_stream_seq[:5])\n",
    "                Two_S5P_up = count_serines(up_stream_seq[5:])\n",
    "                # NOW we assemble the row:\n",
    "                if annotate:\n",
    "                    new_row = [uni_id,gene_name,ref_seq_id,firstAA_position_in_HELIDR,lastAA_position_in_HELIDR,up_stream_seq,HELIDR_seq,\n",
    "                          down_stream_seq,Two_S5P_up,Two_S5P_down,'','','','','','','','','',annotations[i]]\n",
    "                else:\n",
    "                    new_row = [uni_id,gene_name,ref_seq_id,firstAA_position_in_HELIDR,lastAA_position_in_HELIDR,up_stream_seq,HELIDR_seq,\n",
    "                          down_stream_seq,Two_S5P_up,Two_S5P_down,'','','','','','','','','']\n",
    "                df.loc[len(df)] = new_row\n",
    "            else:\n",
    "                # We may want to revisit this continue here\n",
    "                continue\n",
    "        return df, None\n",
    "            \n",
    "    else:\n",
    "        # print(f'sequence length from structure is {seq_length}, from alpha fold {len(sec_struc_output)}')\n",
    "        # print(\"there is an incompatibility between the sequence and alpha fold structure\")\n",
    "        # print(f'the current uniprotid with the issue is: {uni_prot_id}')\n",
    "        return df, uni_prot_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16663482",
   "metadata": {},
   "source": [
    "## Testing things out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "68f0044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('data/unique_ids_from_spreadsheet.txt', 'r')\n",
    "f_content = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "74ea2032",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "for f in f_content:\n",
    "    ids.append(f.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ceb56b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "166454f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinitalise_df(annotations = False):\n",
    "    if annotations:\n",
    "        df = pd.DataFrame(columns=['uniprot_id','gene_name','refseq_id','firstAA_position_in_HELIDR','lastAA_position_in_HELIDR','HELIDR_upstream_seq'\n",
    "                       ,'HELIDR_seq', 'HELIDR_downstream_seq', '2S5P_up', '2S5P_down', '2S5P1_up','2S5P1_down','2S5P1_helix','HEK293T_expressed','NonTMD[3]_TMD[2]_SEC[1]',\n",
    "                       'Non_TMD_classification','4 compartments','TG_CY','TG_SR_nonS', 'annotation'])\n",
    "\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['uniprot_id','gene_name','refseq_id','firstAA_position_in_HELIDR','lastAA_position_in_HELIDR','HELIDR_upstream_seq'\n",
    "                       ,'HELIDR_seq', 'HELIDR_downstream_seq', '2S5P_up', '2S5P_down', '2S5P1_up','2S5P1_down','2S5P1_helix','HEK293T_expressed','NonTMD[3]_TMD[2]_SEC[1]',\n",
    "                       'Non_TMD_classification','4 compartments','TG_CY','TG_SR_nonS'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "39699650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on ID: O00305 this is entry 0/16236\n",
      "Working on ID: Q96CH1 this is entry 200/16236\n",
      "Issue: No gene found found\n",
      "Working on ID: P22090 this is entry 400/16236\n",
      "Working on ID: O15344 this is entry 600/16236\n",
      "Working on ID: Q14697 this is entry 800/16236\n",
      "Working on ID: P24928 this is entry 1000/16236\n",
      "Working on ID: P48067 this is entry 1200/16236\n",
      "Working on ID: P01733 this is entry 1400/16236\n",
      "Working on ID: P51649 this is entry 1600/16236\n",
      "Working on ID: Q8NH83 this is entry 1800/16236\n",
      "Issue: No gene found found\n",
      "Issue: No gene found found\n",
      "Working on ID: Q8WTP8 this is entry 2000/16236\n",
      "Issue: No gene found found\n",
      "Issue: No gene found found\n",
      "Working on ID: Q9H3L0 this is entry 2200/16236\n",
      "Working on ID: Q9UJW7 this is entry 2400/16236\n",
      "Working on ID: Q15596 this is entry 2600/16236\n",
      "Working on ID: Q8N5D6 this is entry 2800/16236\n",
      "Working on ID: P06401 this is entry 3000/16236\n",
      "Working on ID: Q96MD2 this is entry 3200/16236\n",
      "Working on ID: Q16543 this is entry 3400/16236\n",
      "Issue: No gene found found\n",
      "Working on ID: Q09161 this is entry 3600/16236\n",
      "Issue: No gene found found\n",
      "Working on ID: Q96NJ5 this is entry 3800/16236\n",
      "Working on ID: P07954 this is entry 4000/16236\n",
      "Working on ID: Q9GZZ9 this is entry 4200/16236\n",
      "Issue: No gene found found\n",
      "Working on ID: P48553 this is entry 4400/16236\n",
      "Working on ID: Q008S8 this is entry 4600/16236\n",
      "Working on ID: A6NF34 this is entry 4800/16236\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "df = reinitalise_df(annotations= True)\n",
    "for i in ids[0:5000]:\n",
    "    if counter%200 == 0:\n",
    "        print(f'Working on ID: {i} this is entry {counter}/{len(ids)}')\n",
    "    df, fail = generate_table_rows_for_uniprot_id(i,df, annotate=True)\n",
    "    if fail != None:\n",
    "        fail_list.append(fail)\n",
    "    counter = counter+1\n",
    "df.to_csv('part_01_annotated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "559e09f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on ID: Q8IX06 this is entry 5000/16236\n",
      "Working on ID: Q6PL18 this is entry 5200/16236\n",
      "Working on ID: Q8TD19 this is entry 5400/16236\n",
      "Working on ID: Q9BZ71 this is entry 5600/16236\n",
      "Working on ID: Q8IZ08 this is entry 5800/16236\n",
      "Issue: No gene found found\n",
      "Working on ID: P35249 this is entry 6000/16236\n",
      "Issue: No gene found found\n",
      "Working on ID: A8MQ27 this is entry 6200/16236\n",
      "Issue: No gene found found\n",
      "Issue: No gene found found\n",
      "Working on ID: Q9NX36 this is entry 6400/16236\n",
      "Working on ID: O75603 this is entry 6600/16236\n",
      "Working on ID: Q495M3 this is entry 6800/16236\n",
      "Issue: No gene found found\n",
      "Working on ID: Q96B01 this is entry 7000/16236\n",
      "Working on ID: A0A0B4J2F2 this is entry 7200/16236\n",
      "Working on ID: P49747 this is entry 7400/16236\n",
      "Issue: No gene found found\n",
      "Working on ID: Q5JTB6 this is entry 7600/16236\n",
      "Working on ID: Q9UK96 this is entry 7800/16236\n",
      "Issue: No gene found found\n",
      "Working on ID: Q96B26 this is entry 8000/16236\n",
      "Working on ID: Q8N782 this is entry 8200/16236\n",
      "Issue: No gene found found\n",
      "Working on ID: P15018 this is entry 8400/16236\n",
      "Working on ID: Q9BVS4 this is entry 8600/16236\n",
      "Working on ID: A6NEY3 this is entry 8800/16236\n",
      "Working on ID: Q9UKV3 this is entry 9000/16236\n",
      "Working on ID: Q99490 this is entry 9200/16236\n",
      "Working on ID: Q8TAX9 this is entry 9400/16236\n",
      "Working on ID: P13489 this is entry 9600/16236\n",
      "Working on ID: Q6P587 this is entry 9800/16236\n"
     ]
    }
   ],
   "source": [
    "counter = 5000\n",
    "df = reinitalise_df(True)\n",
    "for i in ids[5000:10000]:\n",
    "    if counter%200 == 0:\n",
    "        print(f'Working on ID: {i} this is entry {counter}/{len(ids)}')\n",
    "    df, fail = generate_table_rows_for_uniprot_id(i,df, annotate=True)\n",
    "    if fail != None:\n",
    "        fail_list.append(fail)\n",
    "    counter = counter+1\n",
    "df.to_csv('part_02_annotated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "12343250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on ID: Q8N5Y2 this is entry 10000/16236\n",
      "Issue: No gene found found\n",
      "Issue: No gene found found\n",
      "Working on ID: Q711Q0 this is entry 10200/16236\n",
      "Working on ID: Q9H063 this is entry 10400/16236\n",
      "Working on ID: Q9HCK1 this is entry 10600/16236\n",
      "Working on ID: O60774 this is entry 10800/16236\n",
      "Working on ID: Q8WUK0 this is entry 11000/16236\n",
      "Working on ID: P0DP08 this is entry 11200/16236\n",
      "Working on ID: Q86V35 this is entry 11400/16236\n",
      "Working on ID: O15013 this is entry 11600/16236\n",
      "Working on ID: Q8NGG8 this is entry 11800/16236\n",
      "Issue: No gene found found\n",
      "Issue: No gene found found\n",
      "Working on ID: Q9P2M7 this is entry 12000/16236\n",
      "Working on ID: P01584 this is entry 12200/16236\n",
      "Working on ID: Q6PKG0 this is entry 12400/16236\n",
      "Working on ID: Q08477 this is entry 12600/16236\n",
      "Working on ID: Q3SYA9 this is entry 12800/16236\n",
      "Working on ID: P25789 this is entry 13000/16236\n",
      "Working on ID: A0AV96 this is entry 13200/16236\n",
      "Working on ID: Q6PH85 this is entry 13400/16236\n",
      "Working on ID: Q8NHH9 this is entry 13600/16236\n",
      "Working on ID: Q8WXT5 this is entry 13800/16236\n",
      "Issue: No gene found found\n",
      "Working on ID: Q16611 this is entry 14000/16236\n",
      "Working on ID: P20941 this is entry 14200/16236\n",
      "Issue: No gene found found\n",
      "Working on ID: A8CG34 this is entry 14400/16236\n",
      "Working on ID: P18825 this is entry 14600/16236\n",
      "Working on ID: Q9P2X7 this is entry 14800/16236\n",
      "Issue: No gene found found\n"
     ]
    }
   ],
   "source": [
    "counter = 10000\n",
    "df = reinitalise_df(True)\n",
    "for i in ids[10000:15000]:\n",
    "    if counter%200 == 0:\n",
    "        print(f'Working on ID: {i} this is entry {counter}/{len(ids)}')\n",
    "    df, fail = generate_table_rows_for_uniprot_id(i,df, annotate=True)\n",
    "    if fail != None:\n",
    "        fail_list.append(fail)\n",
    "    counter = counter+1\n",
    "df.to_csv('part_03_annotated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "49ae8661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on ID: P52294 this is entry 15000/16236\n",
      "Working on ID: Q4G0T1 this is entry 15200/16236\n",
      "Working on ID: Q9NPH3 this is entry 15400/16236\n",
      "Issue: No gene found found\n",
      "Working on ID: Q9BTL3 this is entry 15600/16236\n",
      "Working on ID: Q9H8M1 this is entry 15800/16236\n",
      "Working on ID: Q6VMQ6 this is entry 16000/16236\n",
      "Working on ID: Q5VSL9 this is entry 16200/16236\n"
     ]
    }
   ],
   "source": [
    "counter = 15000\n",
    "df = reinitalise_df(True)\n",
    "for i in ids[15000:]:\n",
    "    if counter%200 == 0:\n",
    "        print(f'Working on ID: {i} this is entry {counter}/{len(ids)}')\n",
    "    df, fail = generate_table_rows_for_uniprot_id(i,df,annotate=True)\n",
    "    if fail != None:\n",
    "        fail_list.append(fail)\n",
    "    counter = counter+1\n",
    "df.to_csv('part_04_annotated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fdd9dadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q3KNS1', 'Q96MI6', 'Q99102', 'Q6ZR37', 'P36639', 'Q12983', 'Q9NX55', 'P47756', 'Q96AX9', 'A6NMZ5', 'Q5H9F3', 'Q3ZCN5', 'B4E2M5', 'O43451', 'O43309', 'P17019', 'P0DN82', 'Q13507', 'P02708', 'P20848', 'P26599', 'Q9BV97', 'Q8NDH2', 'Q9BRI3', 'Q8NGU1', 'P23109', 'Q2VWA4', 'Q16635', 'Q6NY19', 'Q8N859', 'Q92782', 'P59826', 'Q6ZTN6', 'P0DMP1', 'Q8N7C3', 'Q6P3W6', 'O15131', 'Q9UPX8', 'Q6ZP01', 'Q8NFD5', 'Q7Z7G0', 'O94851', 'A6NNF4', 'Q8NH41', 'P0C623', 'Q9HD15', 'Q8IZM8', 'O15016', 'Q8N782', 'Q8IV03', 'Q9H310', 'Q03518', 'Q66PJ3', 'Q8TAX9', 'Q8N423', 'O94901', 'Q9HBL0', 'Q3I5F7', 'Q96Q27', 'Q9BY31', 'Q15170', 'Q9UJX3', 'Q12766', 'D6RF30', 'P29973', 'A0A0U1RRI6', 'Q0D2K0', 'Q9P0K9', 'Q9P2M7', 'Q96KE9', 'Q15270', 'A8MZ97', 'P62861', 'Q9Y5I7', 'Q9HBW0', 'Q5XKL5', 'Q53TS8', 'O94880', 'Q16850', 'Q6ZMS7', 'Q8NBS3', 'Q6ZMR5', 'P51606', 'Q9NRJ5', 'Q96FT7', 'Q6PKH6']\n"
     ]
    }
   ],
   "source": [
    "print(fail_list)\n",
    "np.savetxt('data/failed_AF_structures.csv', fail_list, fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c6870f",
   "metadata": {},
   "source": [
    "## Playground testing things out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8589316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_filter = []\n",
    "f = open('domain.csv')\n",
    "f_content = f.readlines()\n",
    "f.close()\n",
    "\n",
    "for l in f_content:\n",
    "    domain_filter.append(l.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ea67df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_filter = []\n",
    "f = open('region_filter.csv')\n",
    "f_content = f.readlines()\n",
    "f.close()\n",
    "\n",
    "for l in f_content:\n",
    "    region_filter.append(l.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "68939b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "discard_list = np.hstack((region_filter,domain_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a8d8f689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ARM-like and Heat-like helical repeats'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discard_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b4523df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on uniprot ID Q9H4S2\n",
      "At entry 0/1\n",
      "ID: Q9H4S2\n",
      "got secondary structure\n",
      "['-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n",
      " '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n",
      " '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' 'T' 'T' '-' '-' 'T' 'T' 'S'\n",
      " '-' '-' '-' '-' 'H' 'H' 'H' 'H' 'H' 'H' 'T' 'T' '-' '-' '-' '-' '-' '-'\n",
      " '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n",
      " '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n",
      " '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n",
      " '-' '-' 'T' 'T' 'S' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n",
      " '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'H' 'H' '-' 'S' 'S' '-' '-' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'H' 'T' 'T' '-' '-' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'H'\n",
      " 'H' 'H' 'H' 'H' 'H' 'H' 'H' 'T' 'T' 'T' 'T' 'T' 'T' '-' '-' 'S' '-' '-'\n",
      " '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n",
      " '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-'\n",
      " '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-' '-']\n",
      "read json file successfully\n"
     ]
    }
   ],
   "source": [
    "uni_prot_id = 'Q9H4S2'\n",
    "work_dir = 'temp'\n",
    "print('Working on uniprot ID', uni_prot_id)\n",
    "# Downloading uniprot file and alphafold file\n",
    "download_uniprot_json_file(uni_prot_id, work_dir)\n",
    "download_alpha_fold_pdbs([uni_prot_id], workdir =work_dir)\n",
    "p = 'temp/AF-'+uni_prot_id+'-F1-model_v2.pdb'\n",
    "sec_struc_output = get_sec_struct(p, '/Users/toni_brain/miniconda3/envs/dssp//bin/mkdssp')\n",
    "print('got secondary structure')\n",
    "print(sec_struc_output)\n",
    "\n",
    "# Process uniprot file\n",
    "f = open('temp/'+uni_prot_id+'.json')\n",
    "data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "print('read json file successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a9cd903f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "       '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "       '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "       '-', '-', '-', '-', '-', '-', '-', '-', 'T', 'T', '-', '-', 'T',\n",
       "       'T', 'S', '-', '-', '-', '-', 'H', 'H', 'H', 'H', 'H', 'H', 'T',\n",
       "       'T', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "       '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "       '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "       '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "       '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'T', 'T',\n",
       "       'S', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "       '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', 'H',\n",
       "       'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', '-',\n",
       "       'S', 'S', '-', '-', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H',\n",
       "       'H', 'T', 'T', '-', '-', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H',\n",
       "       'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'H', 'T', 'T', 'T',\n",
       "       'T', 'T', 'T', '-', '-', 'S', '-', '-', '-', '-', '-', '-', '-',\n",
       "       '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "       '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "       '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-', '-',\n",
       "       '-', '-', '-', '-'], dtype='<U1')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sec_struc_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7c74fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is collecting known information\n",
    "\n",
    "known_things = {}\n",
    "known_things['Domain'] = []\n",
    "known_things['Region'] = []\n",
    "known_things['Motif'] = []\n",
    "#known_things['Helix'] = []\n",
    "\n",
    "for d in data['features']:\n",
    "    #print(d['type'])\n",
    "    if d['type'] == 'Domain':\n",
    "        known_things['Domain'].append([d['location']['start']['value'], d['location']['end']['value'], d['description']])\n",
    "    elif d['type'] == 'Region':\n",
    "        known_things['Region'].append([d['location']['start']['value'], d['location']['end']['value'], d['description']])\n",
    "    elif d['type'] == 'Motif':\n",
    "        known_things['Motif'].append([d['location']['start']['value'], d['location']['end']['value'], d['description']])\n",
    "    #elif d['type'] == 'Helix':\n",
    "    #    known_things['Helix'].append([d['location']['start']['value'], d['location']['end']['value'], d['description']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a0f94460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Domain': [],\n",
       " 'Region': [[1, 20, 'SNAG domain'], [201, 264, 'Disordered']],\n",
       " 'Motif': []}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5ab8af6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'Region', 'location': {'start': {'value': 1, 'modifier': 'EXACT'}, 'end': {'value': 20, 'modifier': 'EXACT'}}, 'description': 'SNAG domain', 'evidences': [{'evidenceCode': 'ECO:0000250'}]}\n",
      "{'type': 'Region', 'location': {'start': {'value': 201, 'modifier': 'EXACT'}, 'end': {'value': 264, 'modifier': 'EXACT'}}, 'description': 'Disordered', 'evidences': [{'evidenceCode': 'ECO:0000256', 'source': 'SAM', 'id': 'MobiDB-lite'}]}\n"
     ]
    }
   ],
   "source": [
    "for d in data['features']:\n",
    "    if d['type']=='Domain':\n",
    "        print (d)\n",
    "    if d['type']=='Region':\n",
    "        print (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a79b93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "sequence_length = data['sequence']['length']\n",
    "if sequence_length == len(sec_struc_output):\n",
    "    print('yes')\n",
    "    bool_array =  np.zeros(sequence_length, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19dd0d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106], [121, 122, 123, 124, 125, 126, 127, 128, 129, 130], [139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153], [175, 176, 177, 178, 179, 180, 181, 182, 183]]\n"
     ]
    }
   ],
   "source": [
    "bool_array =  np.zeros(sequence_length, dtype=bool)\n",
    "discard_list = ['bHLH', 'Leucine-zipper']\n",
    "alpha_helix_index_list = get_alpha_helix_length_and_location(sec_struc_output, min_length=7)\n",
    "helix_list_remove_index = []\n",
    "for key in known_things.keys():\n",
    "    for entry in known_things[key]:\n",
    "        if entry[-1] in discard_list:\n",
    "            bool_array[entry[0]-1:entry[1]] = True\n",
    "            index = 0\n",
    "            for a in alpha_helix_index_list:\n",
    "                curr_helix =  np.zeros(sequence_length, dtype=bool)\n",
    "                curr_helix[a] = True\n",
    "                overalp = (np.sum(np.logical_and(bool_array, curr_helix)))\n",
    "                if overalp>4:\n",
    "                    helix_list_remove_index.append(index)\n",
    "                index = index+1\n",
    "unique_idx = np.unique(helix_list_remove_index)\n",
    "for index in sorted(unique_idx, reverse=True):\n",
    "    del alpha_helix_index_list[index]\n",
    "print (alpha_helix_index_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
